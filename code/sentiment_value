import csv

import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

import nltk
#nltk.download('wordnet')
#nltk.download('averaged_perceptron_tagger')
#nltk.download('sentiwordnet')
from nltk.corpus import sentiwordnet as swn

def sentivalue(doc):
    sentences = nltk.sent_tokenize(doc)
    stokens = [nltk.word_tokenize(sent) for sent in sentences]
    taggedlist=[]
    for stoken in stokens: 
        taggedlist.append(nltk.pos_tag(stoken))
    wnl = nltk.WordNetLemmatizer()

    score_list=[]
    for idx,taggedsent in enumerate(taggedlist):
        score_list.append([])
        for idx2,t in enumerate(taggedsent):
            newtag=''
            lemmatized=wnl.lemmatize(t[0])
            if t[1].startswith('NN'):     newtag='n'
            elif t[1].startswith('JJ'):   newtag='a'
            elif t[1].startswith('V'):    newtag='v'
            elif t[1].startswith('R'):    newtag='r'
            else:   newtag=''       
            if(newtag!=''):  
                synsets = list(swn.senti_synsets(lemmatized, newtag))
                #Getting average of all possible sentiments, as you requested        
                score=0
                if(len(synsets)>0):
                    for syn in synsets:
                        score+=syn.pos_score()-syn.neg_score()
                    score_list[idx].append(score/len(synsets))

    #print(score_list)
    sentence_sentiment=[]

    for score_sent in score_list:  
        if len(score_sent)==0: continue
        sentence_sentiment.append(sum([word_score for word_score in score_sent])/len(score_sent))
    #print("Sentiment for each sentence for:"+doc)
    #print(sentence_sentiment)

    num = 0
    for i in range(len(sentence_sentiment)):
        num = num +sentence_sentiment[i]
    return num




file = open('../dataset/hotel.csv', 'r')
reader =csv.reader(file)
count = 0
star_rate = list()
oreintation = list()
review_id = list()
review_label = list()

for row in reader:
    #print(row[10])
    review_id.append(count)
    star_rate.append(row[2])
    review_label.append(row[29])
    review_content = row[10]
    
    #words = word_tokenize(review_content)
    #words = create_word_features(words)
    #oreintation.append(classifier.classify(words))   
    oreintation.append(sentivalue(row[10]))
    count +=1
    #if count ==100: break
    #print(oreintation)
    #print(star_rate)
    #print(count)
    print(count)

fileObject = open('sentivalue.txt', 'w')  
for ip in range(len(star_rate)):  
    fileObject.write(str(review_id[ip])+ '\t\t\t\t'+ star_rate[ip]+ '\t\t\t\t' + str(oreintation[ip]) + '\t\t\t\t' + review_label[ip])  
    fileObject.write('\n')  
fileObject.close()  
